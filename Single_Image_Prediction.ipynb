{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Single Image Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NubFbHqfniJU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b582175-37ea-4d97-97eb-4f5b35ae8924"
      },
      "source": [
        "!unzip \"/content/drive/My Drive/DSC_2966.zip\" -d \"/content/final image\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/DSC_2966.zip\n",
            "  inflating: /content/final image/DSC_2796.JPG  \n",
            "  inflating: /content/final image/DSC_2797.JPG  \n",
            "  inflating: /content/final image/DSC_2798.JPG  \n",
            "  inflating: /content/final image/DSC_2799.JPG  \n",
            "  inflating: /content/final image/DSC_2800.JPG  \n",
            "  inflating: /content/final image/DSC_2801.JPG  \n",
            "  inflating: /content/final image/DSC_2802.JPG  \n",
            "  inflating: /content/final image/DSC_2803.JPG  \n",
            "  inflating: /content/final image/DSC_2804.JPG  \n",
            "  inflating: /content/final image/DSC_2805.JPG  \n",
            "  inflating: /content/final image/DSC_2806.JPG  \n",
            "  inflating: /content/final image/DSC_2808.JPG  \n",
            "  inflating: /content/final image/DSC_2809.JPG  \n",
            "  inflating: /content/final image/DSC_2810.JPG  \n",
            "  inflating: /content/final image/DSC_2811.JPG  \n",
            "  inflating: /content/final image/DSC_2812.JPG  \n",
            "  inflating: /content/final image/DSC_2813.JPG  \n",
            "  inflating: /content/final image/DSC_2814.JPG  \n",
            "  inflating: /content/final image/DSC_2815.JPG  \n",
            "  inflating: /content/final image/DSC_2816.JPG  \n",
            "  inflating: /content/final image/DSC_2817.JPG  \n",
            "  inflating: /content/final image/DSC_2818.JPG  \n",
            "  inflating: /content/final image/DSC_2820.JPG  \n",
            "  inflating: /content/final image/DSC_2821.JPG  \n",
            "  inflating: /content/final image/DSC_2822.JPG  \n",
            "  inflating: /content/final image/DSC_2823.JPG  \n",
            "  inflating: /content/final image/DSC_2824.JPG  \n",
            "  inflating: /content/final image/DSC_2825.JPG  \n",
            "  inflating: /content/final image/DSC_2826.JPG  \n",
            "  inflating: /content/final image/DSC_2827.JPG  \n",
            "  inflating: /content/final image/DSC_2828.JPG  \n",
            "  inflating: /content/final image/DSC_2829.JPG  \n",
            "  inflating: /content/final image/DSC_2830.JPG  \n",
            "  inflating: /content/final image/DSC_2832.JPG  \n",
            "  inflating: /content/final image/DSC_2833.JPG  \n",
            "  inflating: /content/final image/DSC_2834.JPG  \n",
            "  inflating: /content/final image/DSC_2835.JPG  \n",
            "  inflating: /content/final image/DSC_2836.JPG  \n",
            "  inflating: /content/final image/DSC_2837.JPG  \n",
            "  inflating: /content/final image/DSC_2838.JPG  \n",
            "  inflating: /content/final image/DSC_2839.JPG  \n",
            "  inflating: /content/final image/DSC_2840.JPG  \n",
            "  inflating: /content/final image/DSC_2841.JPG  \n",
            "  inflating: /content/final image/DSC_2842.JPG  \n",
            "  inflating: /content/final image/DSC_2844.JPG  \n",
            "  inflating: /content/final image/DSC_2845.JPG  \n",
            "  inflating: /content/final image/DSC_2846.JPG  \n",
            "  inflating: /content/final image/DSC_2847.JPG  \n",
            "  inflating: /content/final image/DSC_2848.JPG  \n",
            "  inflating: /content/final image/DSC_2849.JPG  \n",
            "  inflating: /content/final image/DSC_2850.JPG  \n",
            "  inflating: /content/final image/DSC_2851.JPG  \n",
            "  inflating: /content/final image/DSC_2852.JPG  \n",
            "  inflating: /content/final image/DSC_2853.JPG  \n",
            "  inflating: /content/final image/DSC_2854.JPG  \n",
            "  inflating: /content/final image/DSC_2856.JPG  \n",
            "  inflating: /content/final image/DSC_2857.JPG  \n",
            "  inflating: /content/final image/DSC_2858.JPG  \n",
            "  inflating: /content/final image/DSC_2859.JPG  \n",
            "  inflating: /content/final image/DSC_2860.JPG  \n",
            "  inflating: /content/final image/DSC_2861.JPG  \n",
            "  inflating: /content/final image/DSC_2862.JPG  \n",
            "  inflating: /content/final image/DSC_2863.JPG  \n",
            "  inflating: /content/final image/DSC_2864.JPG  \n",
            "  inflating: /content/final image/DSC_2865.JPG  \n",
            "  inflating: /content/final image/DSC_2866.JPG  \n",
            "  inflating: /content/final image/DSC_2868.JPG  \n",
            "  inflating: /content/final image/DSC_2869.JPG  \n",
            "  inflating: /content/final image/DSC_2870.JPG  \n",
            "  inflating: /content/final image/DSC_2871.JPG  \n",
            "  inflating: /content/final image/DSC_2872.JPG  \n",
            "  inflating: /content/final image/DSC_2873.JPG  \n",
            "  inflating: /content/final image/DSC_2874.JPG  \n",
            "  inflating: /content/final image/DSC_2875.JPG  \n",
            "  inflating: /content/final image/DSC_2876.JPG  \n",
            "  inflating: /content/final image/DSC_2877.JPG  \n",
            "  inflating: /content/final image/DSC_2878.JPG  \n",
            "  inflating: /content/final image/DSC_2880.JPG  \n",
            "  inflating: /content/final image/DSC_2881.JPG  \n",
            "  inflating: /content/final image/DSC_2882.JPG  \n",
            "  inflating: /content/final image/DSC_2883.JPG  \n",
            "  inflating: /content/final image/DSC_2884.JPG  \n",
            "  inflating: /content/final image/DSC_2885.JPG  \n",
            "  inflating: /content/final image/DSC_2886.JPG  \n",
            "  inflating: /content/final image/DSC_2887.JPG  \n",
            "  inflating: /content/final image/DSC_2888.JPG  \n",
            "  inflating: /content/final image/DSC_2889.JPG  \n",
            "  inflating: /content/final image/DSC_2890.JPG  \n",
            "  inflating: /content/final image/DSC_2892.JPG  \n",
            "  inflating: /content/final image/DSC_2893.JPG  \n",
            "  inflating: /content/final image/DSC_2894.JPG  \n",
            "  inflating: /content/final image/DSC_2895.JPG  \n",
            "  inflating: /content/final image/DSC_2896.JPG  \n",
            "  inflating: /content/final image/DSC_2897.JPG  \n",
            "  inflating: /content/final image/DSC_2898.JPG  \n",
            "  inflating: /content/final image/DSC_2899.JPG  \n",
            "  inflating: /content/final image/DSC_2900.JPG  \n",
            "  inflating: /content/final image/DSC_2901.JPG  \n",
            "  inflating: /content/final image/DSC_2902.JPG  \n",
            "  inflating: /content/final image/DSC_2904.JPG  \n",
            "  inflating: /content/final image/DSC_2905.JPG  \n",
            "  inflating: /content/final image/DSC_2906.JPG  \n",
            "  inflating: /content/final image/DSC_2907.JPG  \n",
            "  inflating: /content/final image/DSC_2908.JPG  \n",
            "  inflating: /content/final image/DSC_2909.JPG  \n",
            "  inflating: /content/final image/DSC_2910.JPG  \n",
            "  inflating: /content/final image/DSC_2911.JPG  \n",
            "  inflating: /content/final image/DSC_2912.JPG  \n",
            "  inflating: /content/final image/DSC_2913.JPG  \n",
            "  inflating: /content/final image/DSC_2914.JPG  \n",
            "  inflating: /content/final image/DSC_2916.JPG  \n",
            "  inflating: /content/final image/DSC_2917.JPG  \n",
            "  inflating: /content/final image/DSC_2918.JPG  \n",
            "  inflating: /content/final image/DSC_2919.JPG  \n",
            "  inflating: /content/final image/DSC_2920.JPG  \n",
            "  inflating: /content/final image/DSC_2921.JPG  \n",
            "  inflating: /content/final image/DSC_2922.JPG  \n",
            "  inflating: /content/final image/DSC_2923.JPG  \n",
            "  inflating: /content/final image/DSC_2924.JPG  \n",
            "  inflating: /content/final image/DSC_2925.JPG  \n",
            "  inflating: /content/final image/DSC_2926.JPG  \n",
            "  inflating: /content/final image/DSC_2928.JPG  \n",
            "  inflating: /content/final image/DSC_2929.JPG  \n",
            "  inflating: /content/final image/DSC_2930.JPG  \n",
            "  inflating: /content/final image/DSC_2931.JPG  \n",
            "  inflating: /content/final image/DSC_2932.JPG  \n",
            "  inflating: /content/final image/DSC_2933.JPG  \n",
            "  inflating: /content/final image/DSC_2934.JPG  \n",
            "  inflating: /content/final image/DSC_2935.JPG  \n",
            "  inflating: /content/final image/DSC_2936.JPG  \n",
            "  inflating: /content/final image/DSC_2937.JPG  \n",
            "  inflating: /content/final image/DSC_2938.JPG  \n",
            "  inflating: /content/final image/DSC_2940.JPG  \n",
            "  inflating: /content/final image/DSC_2941.JPG  \n",
            "  inflating: /content/final image/DSC_2942.JPG  \n",
            "  inflating: /content/final image/DSC_2943.JPG  \n",
            "  inflating: /content/final image/DSC_2944.JPG  \n",
            "  inflating: /content/final image/DSC_2945.JPG  \n",
            "  inflating: /content/final image/DSC_2946.JPG  \n",
            "  inflating: /content/final image/DSC_2947.JPG  \n",
            "  inflating: /content/final image/DSC_2948.JPG  \n",
            "  inflating: /content/final image/DSC_2949.JPG  \n",
            "  inflating: /content/final image/DSC_2950.JPG  \n",
            "  inflating: /content/final image/DSC_2952.JPG  \n",
            "  inflating: /content/final image/DSC_2953.JPG  \n",
            "  inflating: /content/final image/DSC_2954.JPG  \n",
            "  inflating: /content/final image/DSC_2955.JPG  \n",
            "  inflating: /content/final image/DSC_2956.JPG  \n",
            "  inflating: /content/final image/DSC_2957.JPG  \n",
            "  inflating: /content/final image/DSC_2958.JPG  \n",
            "  inflating: /content/final image/DSC_2959.JPG  \n",
            "  inflating: /content/final image/DSC_2960.JPG  \n",
            "  inflating: /content/final image/DSC_2961.JPG  \n",
            "  inflating: /content/final image/DSC_2962.JPG  \n",
            "  inflating: /content/final image/DSC_2964.JPG  \n",
            "  inflating: /content/final image/DSC_2965.JPG  \n",
            "  inflating: /content/final image/DSC_2966.JPG  \n",
            "  inflating: /content/final image/DSC_2967.JPG  \n",
            "  inflating: /content/final image/DSC_2968.JPG  \n",
            "  inflating: /content/final image/DSC_2969.JPG  \n",
            "  inflating: /content/final image/DSC_2970.JPG  \n",
            "  inflating: /content/final image/DSC_2971.JPG  \n",
            "  inflating: /content/final image/DSC_2972.JPG  \n",
            "  inflating: /content/final image/DSC_2973.JPG  \n",
            "  inflating: /content/final image/DSC_2974.JPG  \n",
            "  inflating: /content/final image/DSC_2976.JPG  \n",
            "  inflating: /content/final image/DSC_2977.JPG  \n",
            "  inflating: /content/final image/DSC_2978.JPG  \n",
            "  inflating: /content/final image/DSC_2979.JPG  \n",
            "  inflating: /content/final image/DSC_2980.JPG  \n",
            "  inflating: /content/final image/DSC_2981.JPG  \n",
            "  inflating: /content/final image/DSC_2982.JPG  \n",
            "  inflating: /content/final image/DSC_2983.JPG  \n",
            "  inflating: /content/final image/DSC_2984.JPG  \n",
            "  inflating: /content/final image/DSC_2985.JPG  \n",
            "  inflating: /content/final image/DSC_2986.JPG  \n",
            "  inflating: /content/final image/DSC_2988.JPG  \n",
            "  inflating: /content/final image/DSC_2989.JPG  \n",
            "  inflating: /content/final image/DSC_2990.JPG  \n",
            "  inflating: /content/final image/DSC_2991.JPG  \n",
            "  inflating: /content/final image/DSC_2992.JPG  \n",
            "  inflating: /content/final image/DSC_2993.JPG  \n",
            "  inflating: /content/final image/DSC_2994.JPG  \n",
            "  inflating: /content/final image/DSC_2995.JPG  \n",
            "  inflating: /content/final image/DSC_2996.JPG  \n",
            "  inflating: /content/final image/DSC_2998.JPG  \n",
            "  inflating: /content/final image/DSC_3003.JPG  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MsLu2Gv1X-6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f2ca7c7-6d76-4cbf-cc5c-c7970646f678"
      },
      "source": [
        "print(len(os.listdir('/content/FinaIimage')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDeAaXKtoyjO"
      },
      "source": [
        "import os\n",
        "for i in os.listdir('/content/FinaIimage'):\n",
        "  !mv  '/content/FinaIimage/$i' '/content/drive/My Drive/Final Image'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmrHHobn1jyA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f731d348-4ce7-467b-b21f-1d59c3fd87ea"
      },
      "source": [
        "print(len('/content/drive/My Drive/Final Image'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HRfyciJtIG0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e41b7592-0e29-4402-d1cb-42fd8a3929d3"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "import re\n",
        "import scipy\n",
        "import shutil, sys  \n",
        "\n",
        "from collections import OrderedDict\n",
        "import imageio\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os, pdb\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import threading\n",
        "import time\n",
        "from sklearn import metrics\n",
        "#import utils\n",
        "global n_classes\n",
        "#import triplet_loss as tri\n",
        "import os.path\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "n_classes = 2\n",
        "lr = tf.placeholder(tf.float32)      # Learning rate to be fed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_3wR9OxOGjl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46993bda-6431-4056-dd27-4075a3e7e7fa"
      },
      "source": [
        "import math\n",
        "def truncate(number, digits) -> float:\n",
        "    stepper = 10.0 ** digits\n",
        "    return math.trunc(stepper * number) / stepper\n",
        "  \n",
        "r = truncate(123.249954,3)\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123.249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwkHpkhkXfKk"
      },
      "source": [
        "def activation(x):\n",
        "    return tf.nn.swish(x)\n",
        "    \n",
        "def conv2d(name, l_input, w, b, s, p):\n",
        "    l_input = tf.nn.conv2d(l_input, w, strides=[1,s,s,1], padding=p, name=name)\n",
        "    l_input = l_input+b\n",
        "\n",
        "    return l_input\n",
        "\n",
        "def batchnorm(conv, isTraining, name='bn'):\n",
        "    return tf.layers.batch_normalization(conv, training=isTraining, name=\"bn\"+name)\n",
        "\n",
        "def initializer(in_filters, out_filters, name, k_size=3):\n",
        "    w1 = tf.get_variable(name+\"W\", [k_size, k_size, in_filters, out_filters], initializer=tf.truncated_normal_initializer())\n",
        "    b1 = tf.get_variable(name+\"B\", [out_filters], initializer=tf.truncated_normal_initializer())\n",
        "    return w1, b1\n",
        "\n",
        "\n",
        "def residual_block(in_x, in_filters, out_filters, stride, isDownSampled, name, isTraining, k_size=3):\n",
        "    global ema_gp\n",
        "    # first convolution layer\n",
        "    if isDownSampled:\n",
        "      in_x = tf.nn.avg_pool(in_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "      \n",
        "    x = batchnorm(in_x, isTraining, name=name+'FirstBn')\n",
        "    x = activation(x)\n",
        "    w1, b1 = initializer(in_filters, in_filters, name+\"first_res\", k_size=k_size)\n",
        "    x = conv2d(name+'r1', x, w1, b1, 1, \"SAME\")\n",
        "\n",
        "    # second convolution layer\n",
        "    x = batchnorm(x, isTraining, name=name+'SecondBn')\n",
        "    x = activation(x)\n",
        "    w2, b2 = initializer(in_filters, out_filters, name+\"Second_res\",k_size=k_size)\n",
        "    x = conv2d(name+'r2', x, w2, b2, 1, \"SAME\")\n",
        "    \n",
        "    if in_filters != out_filters:\n",
        "        difference = out_filters - in_filters\n",
        "        left_pad = difference // 2\n",
        "        right_pad = difference - left_pad\n",
        "        identity = tf.pad(in_x, [[0, 0], [0, 0], [0, 0], [left_pad, right_pad]])\n",
        "        return x + identity\n",
        "    else:\n",
        "        return in_x + x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ResNet(_X, isTraining):\n",
        "    global n_classes\n",
        "    w1 = tf.get_variable(\"initWeight\", [7, 7, 3, 96], initializer=tf.truncated_normal_initializer())\n",
        "    b1 = tf.get_variable(\"initBias\", [96], initializer=tf.truncated_normal_initializer())\n",
        "    initx = conv2d('conv1', _X, w1, b1, 4, \"VALID\")\n",
        "    \n",
        "    filters_num = [96,128,256]\n",
        "    block_num = [2,4,3]\n",
        "    l_cnt = 1\n",
        "    x = initx\n",
        "    \n",
        "    # ============Feature extraction network with kernel size 3x3============\n",
        "    \n",
        "    for i in range(len(filters_num)):\n",
        "        for j in range(block_num[i]):\n",
        "          \n",
        "            if ((j==block_num[i]-1) & (i<len(filters_num)-1)):\n",
        "                x = residual_block(x, filters_num[i], filters_num[i+1], 2, True, 'ResidualBlock%d'%(l_cnt), isTraining)\n",
        "                print('[L-%d] Build %dth connection layer %d from %d to %d channels' % (l_cnt, i, j, filters_num[i], filters_num[i+1]))\n",
        "            else:\n",
        "                x = residual_block(x, filters_num[i], filters_num[i], 1, False, 'ResidualBlock%d'%(l_cnt), isTraining)\n",
        "                print('[L-%d] Build %dth residual block %d with %d channels' % (l_cnt,i, j, filters_num[i]))\n",
        "            l_cnt +=1\n",
        "            print(\"first,x\")\n",
        "            print(x.get_shape().as_list())\n",
        "    \n",
        "    layer_33 = x\n",
        "    x = initx\n",
        "    \n",
        "    # ============Feature extraction network with kernel size 5x5============\n",
        "    for i in range(len(filters_num)):\n",
        "        for j in range(block_num[i]):\n",
        "          \n",
        "            if ((j==block_num[i]-1) & (i<len(filters_num)-1)):\n",
        "                x = residual_block(x, filters_num[i], filters_num[i+1], 2, True, 'Residual5Block%d'%(l_cnt), isTraining, k_size=5)\n",
        "                print('[L-%d] Build %dth connection layer %d from %d to %d channels' % (l_cnt, i, j, filters_num[i], filters_num[i+1]))\n",
        "            else:\n",
        "                x = residual_block(x, filters_num[i], filters_num[i], 1, False, 'Residual5Block%d'%(l_cnt), isTraining, k_size=5)\n",
        "                print('[L-%d] Build %dth residual block %d with %d channels' % (l_cnt,i, j, filters_num[i]))\n",
        "            l_cnt +=1\n",
        "    layer_55 = x\n",
        "    print(\"Layer33's shape\", layer_33.get_shape().as_list())\n",
        "    print(\"Layer55's shape\", layer_55.get_shape().as_list())\n",
        "\n",
        "    x = tf.concat([layer_33, layer_55], 3) #3 ,means cocat in chanles\n",
        "    print(x.get_shape().as_list())\n",
        "    #x shape=(128,3,3,256)\n",
        "    # ============ Classifier Learning============\n",
        "    \n",
        "    x_shape = x.get_shape().as_list()\n",
        "    dense1 = x_shape[1]*x_shape[2]*x_shape[3]  #3*3*256\n",
        "    W = tf.get_variable(\"featW\", [dense1, 256], initializer=tf.truncated_normal_initializer()) #shape of (3*3*256, 128)\n",
        "    b = tf.get_variable(\"featB\", [256], initializer=tf.truncated_normal_initializer())  \n",
        "    dense1 = tf.reshape(x, [-1, dense1])   #(128, 256*3*3) kol sora  1,256*3*3  , then 128 pic \n",
        "    print(\"dense\")\n",
        "    print(dense1.get_shape().as_list())\n",
        "\n",
        "    feat = tf.nn.softmax(tf.matmul(dense1, W) + b)  #return 128*128   every pic has 128 feature (because 128 filter)\n",
        "    print(\"feat\")\n",
        "    print(feat.get_shape().as_list())\n",
        "\n",
        "    \n",
        "    with tf.variable_scope('Final'):\n",
        "        x = batchnorm(x, isTraining, name='FinalBn')\n",
        "        x = activation(x)\n",
        "        wo, bo=initializer(filters_num[-1]*2, n_classes, \"FinalOutput\") # shape of 3*3 with 128 chanles and 3adad7om=2 \n",
        "        x = conv2d('final', x, wo, bo, 1, \"SAME\") #x shape of 3*3 with 2 chanles w 3adad7om 128  \n",
        "        print(x.get_shape().as_list())\n",
        "\n",
        "        saliency = tf.argmax(x, 3)\n",
        "        print(\"saly\")\n",
        "        print(saliency.get_shape().as_list())\n",
        "\n",
        "        x=tf.reduce_mean(x, [1, 2])\n",
        "        print(x.get_shape().as_list())\n",
        "        W = tf.get_variable(\"FinalW\", [n_classes, n_classes], initializer=tf.truncated_normal_initializer())\n",
        "        b = tf.get_variable(\"FinalB\", [n_classes], initializer=tf.truncated_normal_initializer())\n",
        "\n",
        "        out = tf.matmul(x, W) + b\n",
        "        print(\"out\")\n",
        "        print(out.get_shape().as_list()) #128  * 2 \n",
        "\n",
        "                            \n",
        "                    \n",
        "\n",
        "    return out, feat, saliency\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#==========================================================================\n",
        "#=============Reading data in multithreading manner========================\n",
        "#==========================================================================\n",
        "def read_labeled_image_list(image_list_file, training_img_dir):\n",
        "\n",
        "    f = open(image_list_file, 'r')\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for line in f:\n",
        "        #print(line)\n",
        "        filename, label = line[:-1].split(' ')\n",
        "        #filename = training_img_dir+filename\n",
        "        filenames.append(filename)\n",
        "        labels.append(int(label))\n",
        "        \n",
        "    return filenames, labels\n",
        "    \n",
        "    \n",
        "def read_images_from_disk(input_queue, size1=64):\n",
        "    label = input_queue[1]\n",
        "    fn=input_queue[0]\n",
        "    file_contents = tf.read_file(input_queue[0])\n",
        "    example = tf.image.decode_jpeg(file_contents, channels=3)\n",
        "    \n",
        "    #example = tf.image.decode_png(file_contents, channels=3, name=\"dataset_image\") # png fo rlfw\n",
        "    example=tf.image.resize_images(example, [size1,size1])\n",
        "    return example, label, fn\n",
        "\n",
        "\n",
        "\n",
        "def setup_inputs(sess, filenames, training_img_dir, image_size=64, crop_size=64, isTest=False, batch_size=128):\n",
        "    \n",
        "    # Read each image file\n",
        "    image_list, label_list = read_labeled_image_list(filenames, training_img_dir)\n",
        "\n",
        "    images = tf.cast(image_list, tf.string)\n",
        "    labels = tf.cast(label_list, tf.int64)\n",
        "     # Makes an input queue\n",
        "    if isTest is False:\n",
        "        isShuffle = True\n",
        "        numThr = 4\n",
        "    else:\n",
        "        isShuffle = False\n",
        "        numThr = 1\n",
        "        \n",
        "    input_queue = tf.train.slice_input_producer([images, labels], shuffle=isShuffle)\n",
        "    image, y,fn = read_images_from_disk(input_queue)\n",
        "\n",
        "    channels = 3\n",
        "    image.set_shape([None, None, channels])\n",
        "        \n",
        "    # Crop and other random augmentations\n",
        "    if isTest is False:\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        image = tf.image.random_saturation(image, .95, 1.05)\n",
        "        image = tf.image.random_brightness(image, .05)\n",
        "        image = tf.image.random_contrast(image, .95, 1.05)\n",
        "        \n",
        "    image = tf.cast(image, tf.float32)/255.0\n",
        "    \n",
        "    image, y,fn = tf.train.batch([image, y, fn], batch_size=batch_size, capacity=batch_size*3, num_threads=numThr, name='labels_and_images')\n",
        "    tf.train.start_queue_runners(sess=sess)\n",
        "\n",
        "    return image, y, fn, len(label_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfxVr41tXStx"
      },
      "source": [
        "img_path = '/content/HV2B5HG25T.jpg' ## Enter image path\n",
        "text_file = open(\"/content/test.txt\", \"w\")\n",
        "text_file.write('%s 0\\n'%(img_path))\n",
        "text_file.close()\n",
        "img_path = '/content/HU9JBEGUW1.jpg' #path of any image\n",
        "text_file = open(\"/content/train.txt\", \"w\")\n",
        "text_file.write('%s 0\\n'%(img_path))\n",
        "text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMoCqd_FLTPI"
      },
      "source": [
        "def softmax(y):\n",
        "  res = []\n",
        "  for i in y:\n",
        "    res.append(np.exp(i) / np.sum(np.exp(i), axis=0))\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J_xAwlTPyqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "198bf96f-7f65-4adc-c8c4-0da69ad5aaa0"
      },
      "source": [
        "\n",
        "n_classes = 2\n",
        "\n",
        "\n",
        "tst = tf.placeholder(tf.bool)\n",
        "iter = tf.placeholder(tf.int32)\n",
        "\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "\n",
        "train_data, train_labels, filelist1, glen1 = setup_inputs(sess, 'train.txt', img_path, batch_size=1)\n",
        "\n",
        "\n",
        "\n",
        "with tf.variable_scope(\"ResNet\",reuse=tf.AUTO_REUSE) as scope:\n",
        "    pred, feat,_ = ResNet(train_data, False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-5d3caa5af46a>:184: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From <ipython-input-2-5d3caa5af46a>:199: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "WARNING:tensorflow:From <ipython-input-2-5d3caa5af46a>:200: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From <ipython-input-2-5d3caa5af46a>:11: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:336: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "[L-1] Build 0th residual block 0 with 96 channels\n",
            "first,x\n",
            "[1, 15, 15, 96]\n",
            "[L-2] Build 0th connection layer 1 from 96 to 128 channels\n",
            "first,x\n",
            "[1, 7, 7, 128]\n",
            "[L-3] Build 1th residual block 0 with 128 channels\n",
            "first,x\n",
            "[1, 7, 7, 128]\n",
            "[L-4] Build 1th residual block 1 with 128 channels\n",
            "first,x\n",
            "[1, 7, 7, 128]\n",
            "[L-5] Build 1th residual block 2 with 128 channels\n",
            "first,x\n",
            "[1, 7, 7, 128]\n",
            "[L-6] Build 1th connection layer 3 from 128 to 256 channels\n",
            "first,x\n",
            "[1, 3, 3, 256]\n",
            "[L-7] Build 2th residual block 0 with 256 channels\n",
            "first,x\n",
            "[1, 3, 3, 256]\n",
            "[L-8] Build 2th residual block 1 with 256 channels\n",
            "first,x\n",
            "[1, 3, 3, 256]\n",
            "[L-9] Build 2th residual block 2 with 256 channels\n",
            "first,x\n",
            "[1, 3, 3, 256]\n",
            "[L-10] Build 0th residual block 0 with 96 channels\n",
            "[L-11] Build 0th connection layer 1 from 96 to 128 channels\n",
            "[L-12] Build 1th residual block 0 with 128 channels\n",
            "[L-13] Build 1th residual block 1 with 128 channels\n",
            "[L-14] Build 1th residual block 2 with 128 channels\n",
            "[L-15] Build 1th connection layer 3 from 128 to 256 channels\n",
            "[L-16] Build 2th residual block 0 with 256 channels\n",
            "[L-17] Build 2th residual block 1 with 256 channels\n",
            "[L-18] Build 2th residual block 2 with 256 channels\n",
            "Layer33's shape [1, 3, 3, 256]\n",
            "Layer55's shape [1, 3, 3, 256]\n",
            "[1, 3, 3, 512]\n",
            "dense\n",
            "[1, 4608]\n",
            "feat\n",
            "[1, 256]\n",
            "[1, 3, 3, 2]\n",
            "saly\n",
            "[1, 3, 3]\n",
            "[1, 2]\n",
            "out\n",
            "[1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2py8bk3293I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "4ca48a77-03de-4e2b-e20f-3de528b6dd2a"
      },
      "source": [
        "#testing\n",
        "\n",
        "tf.global_variables_initializer()\n",
        "#v2 = tf.get_variable(\"v5\", shape=[5])\n",
        "# Add ops to save and restore all the variables.\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "'''with tf.Session() as sess:\n",
        "  new_saver =tf.train.import_meta_graph('/content/drive/My Drive/checkpointsPG/tf_deepUD_tri_model_iter_45001_for_progressGAN.ckpt')\n",
        "'''  \n",
        "''' initialization '''\n",
        "ckpt_dir = '/content/drive/My Drive/exp2/checkpoints/tf_deepUD_tri_model_all.ckpt'\n",
        "saver.restore(sess, ckpt_dir)\n",
        "pth = '/content/test.txt'\n",
        "test_data, test_labels, filelist2test, tlen1test = setup_inputs(sess,pth, img_path, batch_size=1,isTest=True)\n",
        "\n",
        "with tf.variable_scope(\"ResNet\",reuse=tf.AUTO_REUSE) as scope:\n",
        "    #scope.reuse_variables()\n",
        "    testpred, _, saliencyT = ResNet(test_data, False)\n",
        "result=sess.run(testpred)\n",
        "print(\"trace\")\n",
        "result = np.array(result, dtype=np.float128)\n",
        "percentage = softmax(result)\n",
        "print(result)\n",
        "\n",
        "print(percentage)\n",
        "print('Fake!! {}%'.format(percentage[0][0]*100))\n",
        "print('Real   {}%'.format(percentage[0][1]*100))\n",
        "print(percentage[0][0] + percentage[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/exp2/checkpoints/tf_deepUD_tri_model_all.ckpt\n",
            "[L-1] Build 0th residual block 0 with 96 channels\n",
            "first,x\n",
            "[1, 15, 15, 96]\n",
            "[L-2] Build 0th connection layer 1 from 96 to 128 channels\n",
            "first,x\n",
            "[1, 7, 7, 128]\n",
            "[L-3] Build 1th residual block 0 with 128 channels\n",
            "first,x\n",
            "[1, 7, 7, 128]\n",
            "[L-4] Build 1th residual block 1 with 128 channels\n",
            "first,x\n",
            "[1, 7, 7, 128]\n",
            "[L-5] Build 1th residual block 2 with 128 channels\n",
            "first,x\n",
            "[1, 7, 7, 128]\n",
            "[L-6] Build 1th connection layer 3 from 128 to 256 channels\n",
            "first,x\n",
            "[1, 3, 3, 256]\n",
            "[L-7] Build 2th residual block 0 with 256 channels\n",
            "first,x\n",
            "[1, 3, 3, 256]\n",
            "[L-8] Build 2th residual block 1 with 256 channels\n",
            "first,x\n",
            "[1, 3, 3, 256]\n",
            "[L-9] Build 2th residual block 2 with 256 channels\n",
            "first,x\n",
            "[1, 3, 3, 256]\n",
            "[L-10] Build 0th residual block 0 with 96 channels\n",
            "[L-11] Build 0th connection layer 1 from 96 to 128 channels\n",
            "[L-12] Build 1th residual block 0 with 128 channels\n",
            "[L-13] Build 1th residual block 1 with 128 channels\n",
            "[L-14] Build 1th residual block 2 with 128 channels\n",
            "[L-15] Build 1th connection layer 3 from 128 to 256 channels\n",
            "[L-16] Build 2th residual block 0 with 256 channels\n",
            "[L-17] Build 2th residual block 1 with 256 channels\n",
            "[L-18] Build 2th residual block 2 with 256 channels\n",
            "Layer33's shape [1, 3, 3, 256]\n",
            "Layer55's shape [1, 3, 3, 256]\n",
            "[1, 3, 3, 512]\n",
            "dense\n",
            "[1, 4608]\n",
            "feat\n",
            "[1, 256]\n",
            "[1, 3, 3, 2]\n",
            "saly\n",
            "[1, 3, 3]\n",
            "[1, 2]\n",
            "out\n",
            "[1, 2]\n",
            "trace\n",
            "[[-108.73347473 -135.61199951]]\n",
            "[array([1.00000000e+00, 2.12229136e-12], dtype=float128)]\n",
            "Fake!! 99.99999999978778%\n",
            "Real   2.1222913609694716e-10%\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}